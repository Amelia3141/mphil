{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üî• GPU-Accelerated OrdinalSustain Analysis\n",
    "\n",
    "**Google Colab GPU Setup**\n",
    "\n",
    "This notebook runs your OrdinalSustain analysis on GPU, reducing runtime from **30 days ‚Üí 2-4 days**!\n",
    "\n",
    "## ‚ö° Before You Start:\n",
    "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí Select **T4 GPU** (free) or **A100 GPU** (Pro)\n",
    "2. **Run cells in order**: Press `Shift + Enter` on each cell\n",
    "3. **Test first**: Run quick test (Cell 6) before full analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpu-check"
   },
   "source": [
    "## 1Ô∏è‚É£ GPU Detection\n",
    "\n",
    "Check if GPU is available and working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-1"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîç GPU DETECTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check GPU\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"\\n‚úÖ GPU detected!\\n\")\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  GPU may not be enabled.\")\n",
    "        print(\"Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n‚ùå GPU is not available.\")\n",
    "    print(\"Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install"
   },
   "source": [
    "## 2Ô∏è‚É£ Install Dependencies\n",
    "\n",
    "Install all required packages (takes ~2-3 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-2"
   },
   "outputs": [],
   "source": "print(\"=\"*70)\nprint(\"üì¶ INSTALLING DEPENDENCIES\")\nprint(\"=\"*70)\n\n# Install packages (GPU version only - no pathos/dill needed)\nprint(\"\\nüì¶ Installing packages...\")\n!pip install -q torch numpy scipy matplotlib tqdm scikit-learn pandas\n!pip install -q git+https://github.com/noxtoby/awkde.git\n!pip install -q git+https://github.com/ucl-pond/kde_ebm.git\n\n# Verify PyTorch can see GPU\nimport torch\nprint(f\"\\nüîß PyTorch GPU Info:\")\nprint(f\"   ‚Ä¢ PyTorch version: {torch.__version__}\")\nprint(f\"   ‚Ä¢ CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"   ‚Ä¢ CUDA version: {torch.version.cuda}\")\n    print(f\"   ‚Ä¢ GPU device: {torch.cuda.get_device_name(0)}\")\n    print(f\"   ‚Ä¢ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\nelse:\n    print(\"   ‚ö†Ô∏è  CUDA not available. Please enable GPU runtime.\")\n\nprint(\"\\n‚úÖ All dependencies installed!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone"
   },
   "source": [
    "## 3Ô∏è‚É£ Clone Repository\n",
    "\n",
    "Get the latest GPU-optimized code from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-3"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üì• CLONING REPOSITORY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Remove existing directory if present\n",
    "!rm -rf mphil\n",
    "\n",
    "# Clone repository\n",
    "!git clone https://github.com/Amelia3141/mphil.git\n",
    "%cd mphil\n",
    "\n",
    "# Checkout GPU branch with latest optimizations\n",
    "!git checkout claude/optimize-sustain-speed-011CV4Lk8FuUjS6hZNj13WE3\n",
    "\n",
    "# Add to Python path\n",
    "import sys\n",
    "sys.path.insert(0, '/content/mphil')\n",
    "\n",
    "print(\"\\n‚úÖ Repository ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-prep"
   },
   "source": [
    "## 4Ô∏è‚É£ Prepare Your Data\n",
    "\n",
    "**Option A**: Load your real data (uncomment and edit paths below)\n",
    "**Option B**: Use synthetic test data (runs as-is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ============================================================================\n",
    "# OPTION A: Load Your Real Data (Uncomment and edit paths)\n",
    "# ============================================================================\n",
    "# prob_nl = np.load('/content/drive/MyDrive/your_data/prob_nl.npy')\n",
    "# prob_score = np.load('/content/drive/MyDrive/your_data/prob_score.npy')\n",
    "# score_vals = np.load('/content/drive/MyDrive/your_data/score_vals.npy')\n",
    "# biomarker_labels = ['Domain1', 'Domain2', 'Domain3', ...]  # Your labels\n",
    "\n",
    "# ============================================================================\n",
    "# OPTION B: Generate Synthetic Test Data (Default)\n",
    "# ============================================================================\n",
    "def generate_test_data(n_subjects=8000, n_biomarkers=13, n_scores=3, seed=42):\n",
    "    \"\"\"Generate synthetic test data for OrdinalSustain.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Probability distributions\n",
    "    p_correct = 0.9\n",
    "    p_nl_dist = np.full((n_scores + 1), (1 - p_correct) / n_scores)\n",
    "    p_nl_dist[0] = p_correct\n",
    "    \n",
    "    p_score_dist = np.full((n_scores, n_scores + 1), (1 - p_correct) / n_scores)\n",
    "    for score in range(n_scores):\n",
    "        p_score_dist[score, score + 1] = p_correct\n",
    "    \n",
    "    # Generate data\n",
    "    data = np.random.choice(range(n_scores + 1), n_subjects * n_biomarkers,\n",
    "                          replace=True, p=p_nl_dist)\n",
    "    data = data.reshape((n_subjects, n_biomarkers))\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    prob_nl = p_nl_dist[data]\n",
    "    \n",
    "    prob_score = np.zeros((n_subjects, n_biomarkers, n_scores))\n",
    "    for n in range(n_biomarkers):\n",
    "        for z in range(n_scores):\n",
    "            for score in range(n_scores + 1):\n",
    "                prob_score[data[:, n] == score, n, z] = p_score_dist[z, score]\n",
    "    \n",
    "    score_vals = np.tile(np.arange(1, n_scores + 1), (n_biomarkers, 1))\n",
    "    biomarker_labels = [f\"Biomarker_{i}\" for i in range(n_biomarkers)]\n",
    "    \n",
    "    return prob_nl, prob_score, score_vals, biomarker_labels\n",
    "\n",
    "# Generate test data (matches your dataset dimensions)\n",
    "prob_nl, prob_score, score_vals, biomarker_labels = generate_test_data(\n",
    "    n_subjects=8000,    # YOUR dataset size\n",
    "    n_biomarkers=13,    # YOUR number of biomarkers  \n",
    "    n_scores=3          # YOUR severity levels\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Data ready:\")\n",
    "print(f\"   ‚Ä¢ Subjects: {prob_nl.shape[0]}\")\n",
    "print(f\"   ‚Ä¢ Biomarkers: {prob_nl.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Severity levels: {prob_score.shape[2]}\")\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"   ‚Ä¢ prob_nl: {prob_nl.shape}\")\n",
    "print(f\"   ‚Ä¢ prob_score: {prob_score.shape}\")\n",
    "print(f\"   ‚Ä¢ score_vals: {score_vals.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mount-drive"
   },
   "source": [
    "## üíæ (Optional) Mount Google Drive\n",
    "\n",
    "Uncomment and run this cell if you want to:\n",
    "- Load data from Google Drive\n",
    "- Save results to Google Drive\n",
    "- Preserve results after Colab session ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-mount-drive"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# print(\"‚úÖ Google Drive mounted at /content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quick-test"
   },
   "source": [
    "## 5Ô∏è‚É£ Quick Test (IMPORTANT - Run This First!)\n",
    "\n",
    "**‚ö†Ô∏è Run this before the full analysis!**\n",
    "\n",
    "This test will:\n",
    "- ‚úÖ Verify GPU is working\n",
    "- ‚úÖ Measure actual speedup\n",
    "- ‚úÖ Estimate time for full run\n",
    "\n",
    "Takes ~2-5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-5"
   },
   "outputs": [],
   "source": [
    "from pySuStaIn.TorchOrdinalSustain import TorchOrdinalSustain\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üß™ QUICK TEST - GPU Speedup Verification\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create test output directory\n",
    "test_output = \"./test_output\"\n",
    "os.makedirs(test_output, exist_ok=True)\n",
    "\n",
    "# Create GPU instance with small iteration count\n",
    "test_sustain = TorchOrdinalSustain(\n",
    "    prob_nl, \n",
    "    prob_score, \n",
    "    score_vals, \n",
    "    biomarker_labels,\n",
    "    N_startpoints=5,               # Small for testing\n",
    "    N_S_max=1,                     # Single subtype for testing\n",
    "    N_iterations_MCMC=1000,        # Small for quick test\n",
    "    output_folder=test_output,\n",
    "    dataset_name=\"quicktest\",\n",
    "    use_parallel_startpoints=False,\n",
    "    seed=42,\n",
    "    use_gpu=True,                  # ENABLE GPU!\n",
    "    device_id=0\n",
    ")\n",
    "\n",
    "# Check GPU status\n",
    "if test_sustain.use_gpu:\n",
    "    print(\"\\n‚úÖ GPU initialized successfully!\")\n",
    "    print(f\"   ‚Ä¢ Device: {test_sustain.torch_backend.device_manager.device}\")\n",
    "    print(f\"   ‚Ä¢ Expected speedup: 8-15x on T4, 15-25x on A100\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  GPU not available, running on CPU\")\n",
    "    print(\"   Check: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "# Run test\n",
    "print(\"\\nüöÄ Running quick test...\")\n",
    "start_time = time.time()\n",
    "test_sustain.run_sustain_algorithm()\n",
    "test_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Test completed in {test_time:.1f} seconds\")\n",
    "\n",
    "# Estimate full run time\n",
    "full_iterations = 100000\n",
    "test_iterations = 1000\n",
    "estimated_time = test_time * (full_iterations / test_iterations)\n",
    "estimated_hours = estimated_time / 3600\n",
    "estimated_days = estimated_hours / 24\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä PROJECTIONS FOR FULL RUN\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Full run parameters: {full_iterations} MCMC iterations, 25 startpoints, 3 subtypes\")\n",
    "print(f\"\\nEstimated runtime:\")\n",
    "print(f\"   ‚Ä¢ Hours: {estimated_hours:.1f} hours\")\n",
    "print(f\"   ‚Ä¢ Days: {estimated_days:.1f} days\")\n",
    "\n",
    "if estimated_days < 30:\n",
    "    speedup = 30 / estimated_days\n",
    "    time_saved = 30 - estimated_days\n",
    "    print(f\"\\n‚ö° GPU Speedup:\")\n",
    "    print(f\"   ‚Ä¢ {speedup:.1f}x faster than CPU (30 days)\")\n",
    "    print(f\"   ‚Ä¢ Time saved: {time_saved:.1f} days\")\n",
    "    print(f\"\\n‚úÖ GPU is working! Ready for full analysis.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Warning: Estimated time seems slow. GPU may not be active.\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keep-alive"
   },
   "source": [
    "## üîÑ Keep Colab Alive (For Multi-Day Runs)\n",
    "\n",
    "**‚ö†Ô∏è Colab disconnects after ~12 hours of inactivity!**\n",
    "\n",
    "### Option 1: Auto-Click Connect Button\n",
    "1. Open browser console: Press `F12` (Chrome/Firefox) or `Cmd+Option+J` (Mac)\n",
    "2. Paste this code and press Enter:\n",
    "```javascript\n",
    "function ClickConnect(){\n",
    "  console.log(\"Clicking connect...\");\n",
    "  document.querySelector(\"colab-connect-button\").click();\n",
    "}\n",
    "setInterval(ClickConnect, 60000); // Click every minute\n",
    "```\n",
    "\n",
    "### Option 2: Colab Pro/Pro+ (Recommended for Multi-Day)\n",
    "- **Colab Pro** ($10/month): Longer sessions, better GPUs\n",
    "- **Colab Pro+** ($50/month): Background execution, longest sessions\n",
    "\n",
    "### Option 3: Run Cell Below (Keep Output Active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-keep-alive"
   },
   "outputs": [],
   "source": [
    "# This helps prevent disconnection by keeping output active\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "\n",
    "print(\"‚úÖ Output manager enabled - helps prevent disconnection\")\n",
    "print(\"üí° Still recommended: Use browser console auto-click (see above)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "full-run"
   },
   "source": [
    "## 6Ô∏è‚É£ Full GPU-Accelerated Analysis\n",
    "\n",
    "**üö® BEFORE RUNNING:**\n",
    "1. ‚úÖ Verify quick test (Cell 5) showed good speedup\n",
    "2. ‚úÖ Set up keep-alive (Cell above)\n",
    "3. ‚úÖ Consider Colab Pro/Pro+ for multi-day runs\n",
    "4. ‚úÖ (Optional) Change output folder to Google Drive for persistent storage\n",
    "\n",
    "**‚è∞ This will take 2-4 days even on GPU!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-6"
   },
   "outputs": [],
   "source": [
    "from pySuStaIn.TorchOrdinalSustain import TorchOrdinalSustain\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üî¨ FULL GPU-ACCELERATED ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Output folder (change to Google Drive if mounted)\n",
    "output_folder = \"./gpu_sustain_output\"  \n",
    "# output_folder = \"/content/drive/MyDrive/sustain_output\"  # Uncomment for Google Drive\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Create GPU instance with full parameters\n",
    "gpu_sustain = TorchOrdinalSustain(\n",
    "    prob_nl, \n",
    "    prob_score, \n",
    "    score_vals, \n",
    "    biomarker_labels,\n",
    "    N_startpoints=25,              # Full startpoints\n",
    "    N_S_max=3,                     # 3 subtypes\n",
    "    N_iterations_MCMC=100000,      # Full MCMC iterations\n",
    "    output_folder=output_folder,\n",
    "    dataset_name=\"ordinal_gpu_analysis\",\n",
    "    use_parallel_startpoints=False,\n",
    "    seed=42,\n",
    "    use_gpu=True,                  # GPU ENABLED\n",
    "    device_id=0\n",
    ")\n",
    "\n",
    "# Verify GPU\n",
    "if gpu_sustain.use_gpu:\n",
    "    print(\"\\n‚úÖ GPU confirmed active!\")\n",
    "    print(f\"   ‚Ä¢ Device: {gpu_sustain.torch_backend.device_manager.device}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: GPU not available, will use CPU (very slow!)\")\n",
    "    response = input(\"Continue anyway? (yes/no): \")\n",
    "    if response.lower() != 'yes':\n",
    "        raise RuntimeError(\"GPU not available. Please enable GPU runtime.\")\n",
    "\n",
    "# Show estimated runtime from quick test\n",
    "try:\n",
    "    print(f\"\\n‚è∞ Estimated runtime: ~{estimated_days:.1f} days\")\n",
    "except:\n",
    "    print(\"\\n‚è∞ Estimated runtime: ~2-4 days on T4 GPU, ~1.5-2 days on A100\")\n",
    "\n",
    "print(\"\\nüö® IMPORTANT:\")\n",
    "print(\"   ‚Ä¢ Keep this tab/window open\")\n",
    "print(\"   ‚Ä¢ Keep browser console auto-click running (if using)\")\n",
    "print(\"   ‚Ä¢ Consider Colab Pro/Pro+ for better reliability\")\n",
    "print(\"   ‚Ä¢ Results automatically saved to pickle files\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "input(\"Press Enter to start full analysis...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# START THE ANALYSIS\n",
    "start_time = time.time()\n",
    "start_timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"\\nüöÄ Started at: {start_timestamp}\")\n",
    "print(\"\\nRunning... (this will take days)\\n\")\n",
    "\n",
    "# RUN!\n",
    "samples_sequence, samples_f, ml_subtype, prob_ml_subtype, \\\n",
    "ml_stage, prob_ml_stage, prob_subtype_stage = gpu_sustain.run_sustain_algorithm()\n",
    "\n",
    "# Calculate runtime\n",
    "end_time = time.time()\n",
    "end_timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "runtime = end_time - start_time\n",
    "runtime_str = str(timedelta(seconds=int(runtime)))\n",
    "runtime_hours = runtime / 3600\n",
    "runtime_days = runtime_hours / 24\n",
    "\n",
    "# Results summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Started:  {start_timestamp}\")\n",
    "print(f\"Finished: {end_timestamp}\")\n",
    "print(f\"Runtime:  {runtime_str} ({runtime_hours:.1f} hours = {runtime_days:.1f} days)\")\n",
    "print(f\"\\nResults saved to: {output_folder}\")\n",
    "print(\"\\nüìä Output files:\")\n",
    "!ls -lh {output_folder}\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download-results"
   },
   "source": [
    "## 7Ô∏è‚É£ Download Results\n",
    "\n",
    "After analysis completes, download the results to your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "print(\"üì¶ Preparing results for download...\")\n",
    "\n",
    "# Create zip file\n",
    "output_folder = \"./gpu_sustain_output\"  # Match the folder from Cell 6\n",
    "zip_filename = \"sustain_results\"\n",
    "\n",
    "if os.path.exists(output_folder):\n",
    "    shutil.make_archive(zip_filename, 'zip', output_folder)\n",
    "    print(f\"\\n‚úÖ Results packaged: {zip_filename}.zip\")\n",
    "    print(f\"   Size: {os.path.getsize(zip_filename + '.zip') / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Download\n",
    "    print(\"\\nüì• Starting download...\")\n",
    "    files.download(f\"{zip_filename}.zip\")\n",
    "    print(\"‚úÖ Download complete!\")\n",
    "else:\n",
    "    print(f\"‚ùå Output folder not found: {output_folder}\")\n",
    "    print(\"   Make sure analysis has completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "footer"
   },
   "source": [
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "- **SuStaIn Documentation**: [pySuStaIn GitHub](https://github.com/ucl-pond/pySuStaIn)\n",
    "- **Google Colab Tips**: [Research Colab FAQ](https://research.google.com/colaboratory/faq.html)\n",
    "- **GPU Optimization**: See `TorchOrdinalSustain.py` in the repository\n",
    "\n",
    "## üÜò Troubleshooting\n",
    "\n",
    "**GPU not detected?**\n",
    "- Runtime ‚Üí Change runtime type ‚Üí Select GPU ‚Üí Save\n",
    "- Restart runtime and re-run from Cell 1\n",
    "\n",
    "**Session disconnected?**\n",
    "- Results are saved in pickle files automatically\n",
    "- Reload and check output folder for partial results\n",
    "- Use browser console auto-click (see Cell above)\n",
    "\n",
    "**Out of memory?**\n",
    "- Try reducing `N_startpoints` to 10-15\n",
    "- Upgrade to Colab Pro for more RAM\n",
    "\n",
    "**Too slow?**\n",
    "- Verify GPU is active (check Cell 5 output)\n",
    "- Try A100 GPU (Colab Pro)\n",
    "- Check CUDA is being used in test output\n",
    "\n",
    "---\n",
    "\n",
    "**Created by**: GPU-optimized SuStaIn pipeline  \n",
    "**Version**: TorchOrdinalSustain with CUDA acceleration  \n",
    "**Last Updated**: 2025-11-17"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}