{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# SuStaIn GPU Benchmark\n", "**Instructions:** Runtime → Change runtime type → T4 GPU"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q numpy scipy matplotlib scikit-learn pandas tqdm pathos\n",
    "\n",
    "import os\n",
    "if os.path.exists('mphil'):\n",
    "    %cd mphil\n",
    "    !git pull origin main\n",
    "else:\n",
    "    !git clone https://github.com/Amelia3141/mphil.git\n",
    "    %cd mphil\n",
    "\n",
    "import torch\n",
    "print(f'CUDA: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, os, shutil, sys\n",
    "sys.path.insert(0, 'pySuStaIn')\n",
    "\n",
    "def generate_test_data(n_patients, n_biomarkers, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    data = np.random.randint(0, 4, (n_patients, n_biomarkers))\n",
    "    prob_nl = np.where(data == 0, 0.85, 0.0375)\n",
    "    prob_score = np.zeros((n_patients, n_biomarkers, 3))\n",
    "    for b in range(n_biomarkers):\n",
    "        for z in range(3):\n",
    "            prob_score[:, b, z] = np.where(data[:, b] == z+1, 0.85, 0.0375)\n",
    "    return prob_nl, prob_score, np.tile(np.arange(1,4), (n_biomarkers,1)), [f'B{i}' for i in range(n_biomarkers)]\n",
    "\n",
    "def benchmark(n_patients, n_biomarkers, use_gpu):\n",
    "    from pySuStaIn import OrdinalSustain\n",
    "    from pySuStaIn.TorchOrdinalSustain import TorchOrdinalSustain\n",
    "    prob_nl, prob_score, score_vals, labels = generate_test_data(n_patients, n_biomarkers)\n",
    "    folder = f'./bench_{\"gpu\" if use_gpu else \"cpu\"}'\n",
    "    if os.path.exists(folder): shutil.rmtree(folder)\n",
    "    os.makedirs(folder)\n",
    "    if use_gpu:\n",
    "        model = TorchOrdinalSustain(prob_nl, prob_score, score_vals, labels, 5, 1, 2000, folder, 'b', True, 42, use_gpu=True)\n",
    "    else:\n",
    "        model = OrdinalSustain(prob_nl, prob_score, score_vals, labels, 5, 1, 2000, folder, 'b', False, 42)\n",
    "    start = time.time()\n",
    "    model.run_sustain_algorithm(plot=False)\n",
    "    return time.time() - start\n",
    "\n",
    "print('Setup complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for n in [200, 500, 1000]:\n",
    "    print(f'\\n=== {n} patients, 15 biomarkers ===')\n",
    "    t_cpu = benchmark(n, 15, False)\n",
    "    print(f'CPU: {t_cpu:.1f}s')\n",
    "    t_gpu = benchmark(n, 15, True)\n",
    "    print(f'GPU: {t_gpu:.1f}s')\n",
    "    speedup = t_cpu/t_gpu\n",
    "    print(f'Speedup: {speedup:.2f}x')\n",
    "    results.append({'n_patients': n, 'cpu_sec': t_cpu, 'gpu_sec': t_gpu, 'speedup': speedup})\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "print('\\n' + '='*50)\n",
    "print('BENCHMARK SUMMARY')\n",
    "print('='*50)\n",
    "print(df.to_string(index=False))\n",
    "df.to_csv('benchmark_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
