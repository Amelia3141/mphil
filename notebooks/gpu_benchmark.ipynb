{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# SuStaIn GPU Benchmark\n", "**Instructions:** Runtime → Change runtime type → T4 GPU\n", "\n", "This benchmarks the GPU-batched MCMC implementation against CPU."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q numpy scipy matplotlib scikit-learn pandas tqdm pathos\n",
    "\n",
    "import os\n",
    "if os.path.exists('mphil'):\n",
    "    %cd mphil\n",
    "    !git pull origin main\n",
    "else:\n",
    "    !git clone https://github.com/Amelia3141/mphil.git\n",
    "    %cd mphil\n",
    "\n",
    "import torch\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, os, shutil, sys\n",
    "sys.path.insert(0, 'pySuStaIn')\n",
    "\n",
    "def generate_test_data(n_patients, n_biomarkers, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    data = np.random.randint(0, 4, (n_patients, n_biomarkers))\n",
    "    prob_nl = np.where(data == 0, 0.85, 0.0375)\n",
    "    prob_score = np.zeros((n_patients, n_biomarkers, 3))\n",
    "    for b in range(n_biomarkers):\n",
    "        for z in range(3):\n",
    "            prob_score[:, b, z] = np.where(data[:, b] == z+1, 0.85, 0.0375)\n",
    "    return prob_nl, prob_score, np.tile(np.arange(1,4), (n_biomarkers,1)), [f'B{i}' for i in range(n_biomarkers)]\n",
    "\n",
    "def benchmark(n_patients, n_biomarkers, n_mcmc, use_gpu):\n",
    "    from pySuStaIn import OrdinalSustain\n",
    "    from pySuStaIn.TorchOrdinalSustain import TorchOrdinalSustain\n",
    "    \n",
    "    prob_nl, prob_score, score_vals, labels = generate_test_data(n_patients, n_biomarkers)\n",
    "    folder = f'./bench_{\"gpu\" if use_gpu else \"cpu\"}'\n",
    "    if os.path.exists(folder): shutil.rmtree(folder)\n",
    "    os.makedirs(folder)\n",
    "    \n",
    "    if use_gpu:\n",
    "        model = TorchOrdinalSustain(prob_nl, prob_score, score_vals, labels, 5, 1, n_mcmc, folder, 'b', True, 42, use_gpu=True)\n",
    "    else:\n",
    "        model = OrdinalSustain(prob_nl, prob_score, score_vals, labels, 5, 1, n_mcmc, folder, 'b', False, 42)\n",
    "    \n",
    "    start = time.time()\n",
    "    model.run_sustain_algorithm(plot=False)\n",
    "    return time.time() - start\n",
    "\n",
    "print('Setup complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test first\n",
    "print('Quick test (100 patients, 2000 MCMC)...')\n",
    "t_cpu = benchmark(100, 10, 2000, False)\n",
    "print(f'CPU: {t_cpu:.1f}s')\n",
    "t_gpu = benchmark(100, 10, 2000, True)\n",
    "print(f'GPU: {t_gpu:.1f}s')\n",
    "print(f'Speedup: {t_cpu/t_gpu:.2f}x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full benchmark\n",
    "results = []\n",
    "configs = [\n",
    "    (200, 15, 10000),\n",
    "    (500, 20, 10000),\n",
    "    (1000, 20, 10000),\n",
    "]\n",
    "\n",
    "for n_patients, n_bio, n_mcmc in configs:\n",
    "    print(f'\\n=== {n_patients} patients, {n_bio} biomarkers, {n_mcmc} MCMC ===')\n",
    "    t_cpu = benchmark(n_patients, n_bio, n_mcmc, False)\n",
    "    print(f'CPU: {t_cpu:.1f}s')\n",
    "    t_gpu = benchmark(n_patients, n_bio, n_mcmc, True)\n",
    "    print(f'GPU: {t_gpu:.1f}s')\n",
    "    speedup = t_cpu/t_gpu\n",
    "    print(f'Speedup: {speedup:.2f}x')\n",
    "    results.append({'n_patients': n_patients, 'n_biomarkers': n_bio, 'n_mcmc': n_mcmc, \n",
    "                    'cpu_sec': t_cpu, 'gpu_sec': t_gpu, 'speedup': speedup})\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "print('\\n' + '='*60)\n",
    "print('BENCHMARK SUMMARY')\n",
    "print('='*60)\n",
    "print(df.to_string(index=False))\n",
    "df.to_csv('benchmark_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
