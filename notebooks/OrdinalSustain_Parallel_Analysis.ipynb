{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OrdinalSustain Analysis with Process-Based Parallel MCMC\n",
    "\n",
    "This notebook runs OrdinalSustain with **2-4x speedup** using process-based parallel MCMC.\n",
    "\n",
    "## ‚ö° Key Features:\n",
    "- ‚úÖ **2-4x faster** than standard OrdinalSustain\n",
    "- ‚úÖ **No GPU required** - works on any multi-core CPU\n",
    "- ‚úÖ **Easy to use** - just change 3 parameters\n",
    "- ‚úÖ **Escapes Python's GIL** - uses true multiprocessing\n",
    "\n",
    "## üìä Expected Performance:\n",
    "- **Your 30-day run** ‚Üí ~8-12 days with 4 CPU cores\n",
    "- **Time saved:** ~18-22 days!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "# Import ParallelOrdinalSustain (with process-based parallel MCMC)\n",
    "from pySuStaIn.ParallelOrdinalSustain import ParallelOrdinalSustain\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "\n",
    "# Check system info\n",
    "n_cores = os.cpu_count()\n",
    "print(f\"\\nüíª System Info:\")\n",
    "print(f\"   ‚Ä¢ Available CPU cores: {n_cores}\")\n",
    "print(f\"   ‚Ä¢ Recommended n_mcmc_chains: {min(4, n_cores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load Your Data\n",
    "\n",
    "Replace the file paths below with your actual data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1: Load from .npy files\n",
    "# Uncomment and edit these lines:\n",
    "\n",
    "# prob_nl = np.load('path/to/your/prob_nl.npy')\n",
    "# prob_score = np.load('path/to/your/prob_score.npy')\n",
    "# score_vals = np.load('path/to/your/score_vals.npy')\n",
    "# biomarker_labels = ['Domain1', 'Domain2', 'Domain3', ...]  # Your symptom domains\n",
    "\n",
    "# OPTION 2: Load from CSV/Excel and prepare\n",
    "# Uncomment and edit:\n",
    "\n",
    "# data = pd.read_csv('path/to/your/data.csv')\n",
    "# # ... your data preparation code ...\n",
    "# prob_nl = ...        # Shape: (n_subjects, n_biomarkers)\n",
    "# prob_score = ...     # Shape: (n_subjects, n_biomarkers, n_scores)\n",
    "# score_vals = ...     # Shape: (n_biomarkers, n_scores)\n",
    "# biomarker_labels = ...\n",
    "\n",
    "# OPTION 3: For testing - generate synthetic data\n",
    "print(\"‚ö†Ô∏è  Using synthetic test data. Replace this with your real data!\")\n",
    "\n",
    "def generate_test_data(n_subjects=8000, n_biomarkers=13, n_scores=3, seed=42):\n",
    "    \"\"\"Generate synthetic test data matching your dataset size.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    p_correct = 0.9\n",
    "    p_nl_dist = np.full((n_scores + 1), (1 - p_correct) / n_scores)\n",
    "    p_nl_dist[0] = p_correct\n",
    "    \n",
    "    p_score_dist = np.full((n_scores, n_scores + 1), (1 - p_correct) / n_scores)\n",
    "    for score in range(n_scores):\n",
    "        p_score_dist[score, score + 1] = p_correct\n",
    "    \n",
    "    data = np.random.choice(range(n_scores + 1), n_subjects * n_biomarkers,\n",
    "                          replace=True, p=p_nl_dist)\n",
    "    data = data.reshape((n_subjects, n_biomarkers))\n",
    "    \n",
    "    prob_nl = p_nl_dist[data]\n",
    "    \n",
    "    prob_score = np.zeros((n_subjects, n_biomarkers, n_scores))\n",
    "    for n in range(n_biomarkers):\n",
    "        for z in range(n_scores):\n",
    "            for score in range(n_scores + 1):\n",
    "                prob_score[data[:, n] == score, n, z] = p_score_dist[z, score]\n",
    "    \n",
    "    score_vals = np.tile(np.arange(1, n_scores + 1), (n_biomarkers, 1))\n",
    "    biomarker_labels = [f\"SymptomDomain_{i+1}\" for i in range(n_biomarkers)]\n",
    "    \n",
    "    return prob_nl, prob_score, score_vals, biomarker_labels\n",
    "\n",
    "# Generate test data\n",
    "prob_nl, prob_score, score_vals, biomarker_labels = generate_test_data(\n",
    "    n_subjects=8000,    # YOUR dataset size\n",
    "    n_biomarkers=13,    # YOUR number of biomarkers\n",
    "    n_scores=3          # YOUR number of severity levels\n",
    ")\n",
    "\n",
    "# Verify data\n",
    "print(f\"\\n‚úÖ Data loaded:\")\n",
    "print(f\"   ‚Ä¢ Subjects: {prob_nl.shape[0]}\")\n",
    "print(f\"   ‚Ä¢ Biomarkers: {prob_nl.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Severity levels: {prob_score.shape[2]}\")\n",
    "print(f\"   ‚Ä¢ prob_nl shape: {prob_nl.shape}\")\n",
    "print(f\"   ‚Ä¢ prob_score shape: {prob_score.shape}\")\n",
    "print(f\"   ‚Ä¢ score_vals shape: {score_vals.shape}\")\n",
    "print(f\"\\nüìã Biomarker labels: {biomarker_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Quick Test (Recommended First)\n",
    "\n",
    "Run a **quick test** with fewer iterations to verify everything works and estimate speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üß™ QUICK TEST - Estimating Speedup\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create output directory\n",
    "test_output = \"./test_output\"\n",
    "os.makedirs(test_output, exist_ok=True)\n",
    "\n",
    "# Run quick test with parallel MCMC\n",
    "test_sustain = ParallelOrdinalSustain(\n",
    "    prob_nl=prob_nl,\n",
    "    prob_score=prob_score,\n",
    "    score_vals=score_vals,\n",
    "    biomarker_labels=biomarker_labels,\n",
    "    N_startpoints=5,               # Fewer for testing\n",
    "    N_S_max=1,                     # Just 1 subtype for speed\n",
    "    N_iterations_MCMC=1000,        # Small number to test (~1% of your full run)\n",
    "    output_folder=test_output,\n",
    "    dataset_name=\"quicktest\",\n",
    "    use_parallel_startpoints=False,\n",
    "    seed=42,\n",
    "    # PARALLEL MCMC SETTINGS:\n",
    "    use_parallel_mcmc=True,        # Enable parallel MCMC\n",
    "    n_mcmc_chains=4,               # 4 chains in parallel\n",
    "    mcmc_backend='process'         # Process-based (TRUE parallelism!)\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Running quick test...\")\n",
    "start = time.time()\n",
    "test_sustain.run_sustain_algorithm()\n",
    "test_time = time.time() - start\n",
    "\n",
    "print(f\"\\n‚úÖ Test completed in {test_time:.1f} seconds\")\n",
    "\n",
    "# Estimate full run time\n",
    "full_iterations = 100000  # Your actual MCMC iterations\n",
    "test_iterations = 1000\n",
    "estimated_time = test_time * (full_iterations / test_iterations)\n",
    "estimated_hours = estimated_time / 3600\n",
    "estimated_days = estimated_hours / 24\n",
    "\n",
    "print(f\"\\nüìä Projections for full run ({full_iterations} iterations):\")\n",
    "print(f\"   ‚Ä¢ Estimated time: {estimated_hours:.1f} hours = {estimated_days:.1f} days\")\n",
    "print(f\"   ‚Ä¢ Compare to your original: 30 days\")\n",
    "if estimated_days < 30:\n",
    "    print(f\"   ‚Ä¢ ‚ö° Speedup achieved: {30/estimated_days:.1f}x faster!\")\n",
    "    print(f\"   ‚Ä¢ ‚è∞ Time saved: {30 - estimated_days:.1f} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Full Analysis with Parallel MCMC\n",
    "\n",
    "Once the test looks good, run your **full analysis** here.\n",
    "\n",
    "**‚ö†Ô∏è Important:** This cell will run for several days! Make sure to:\n",
    "- Keep your computer running\n",
    "- Disable sleep/hibernation\n",
    "- Consider using `nohup` or `screen` if running on a server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üî¨ FULL ANALYSIS - Process-Based Parallel MCMC\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create output directory\n",
    "output_folder = \"./sustain_output\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Initialize ParallelOrdinalSustain\n",
    "sustain = ParallelOrdinalSustain(\n",
    "    prob_nl=prob_nl,\n",
    "    prob_score=prob_score,\n",
    "    score_vals=score_vals,\n",
    "    biomarker_labels=biomarker_labels,\n",
    "    \n",
    "    # YOUR ANALYSIS PARAMETERS:\n",
    "    N_startpoints=25,              # Number of initialization points\n",
    "    N_S_max=3,                     # Maximum number of subtypes to test\n",
    "    N_iterations_MCMC=100000,      # Your full MCMC iterations\n",
    "    \n",
    "    output_folder=output_folder,\n",
    "    dataset_name=\"ordinal_analysis\",\n",
    "    use_parallel_startpoints=False,\n",
    "    seed=42,\n",
    "    \n",
    "    # PARALLEL MCMC SETTINGS (THIS IS THE KEY!):\n",
    "    use_parallel_mcmc=True,        # Enable parallel MCMC\n",
    "    n_mcmc_chains=4,               # Number of parallel chains (adjust based on your CPUs)\n",
    "    mcmc_backend='process',        # MUST be 'process' not 'thread'!\n",
    "    parallel_workers=None          # Auto-detect CPU cores (or set manually)\n",
    ")\n",
    "\n",
    "print(f\"\\nüìã Analysis Configuration:\")\n",
    "print(f\"   ‚Ä¢ Dataset: {prob_nl.shape[0]} subjects, {prob_nl.shape[1]} biomarkers\")\n",
    "print(f\"   ‚Ä¢ Max subtypes: {sustain.N_S_max}\")\n",
    "print(f\"   ‚Ä¢ MCMC iterations: {sustain.N_iterations_MCMC:,}\")\n",
    "print(f\"   ‚Ä¢ Parallel chains: {sustain.n_mcmc_chains}\")\n",
    "print(f\"   ‚Ä¢ Backend: {sustain.mcmc_backend}\")\n",
    "\n",
    "print(f\"\\n‚è∞ Estimated runtime: ~{estimated_days:.1f} days\")\n",
    "print(\"\\nüöÄ Starting full analysis...\")\n",
    "print(\"   (This will take several days. Cell will show [*] while running)\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "start_timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"Started at: {start_timestamp}\")\n",
    "\n",
    "# RUN THE ANALYSIS\n",
    "samples_sequence, samples_f, ml_subtype, prob_ml_subtype, \\\n",
    "ml_stage, prob_ml_stage, prob_subtype_stage = sustain.run_sustain_algorithm()\n",
    "\n",
    "# Calculate runtime\n",
    "end_time = time.time()\n",
    "end_timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "runtime = end_time - start_time\n",
    "runtime_str = str(timedelta(seconds=int(runtime)))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Started:  {start_timestamp}\")\n",
    "print(f\"Finished: {end_timestamp}\")\n",
    "print(f\"Runtime:  {runtime_str}\")\n",
    "print(f\"\\nResults saved to: {output_folder}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Analyze Results\n",
    "\n",
    "View and interpret the SuStaIn output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä RESULTS ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check which subtypes model fits best\n",
    "N_S_max = sustain.N_S_max\n",
    "\n",
    "# Plot MCMC likelihood traces\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "for s in range(N_S_max):\n",
    "    pickle_file = f\"{output_folder}/pickle_files/ordinal_analysis_subtype{s}.pickle\"\n",
    "    \n",
    "    if os.path.exists(pickle_file):\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        samples_likelihood = data[\"samples_likelihood\"]\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(samples_likelihood, label=f\"{s+1} subtype(s)\", alpha=0.7)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.hist(samples_likelihood, bins=50, alpha=0.5, label=f\"{s+1} subtype(s)\")\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.xlabel('MCMC Iteration')\n",
    "plt.ylabel('Log Likelihood')\n",
    "plt.title('MCMC Trace')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.xlabel('Log Likelihood')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Likelihood Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_folder}/mcmc_diagnostics.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Diagnostic plots saved to: {output_folder}/mcmc_diagnostics.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtype/Stage assignments\n",
    "print(\"\\nüìã Subject Assignments:\")\n",
    "print(f\"   ‚Ä¢ ML subtypes shape: {ml_subtype.shape}\")\n",
    "print(f\"   ‚Ä¢ ML stages shape: {ml_stage.shape}\")\n",
    "\n",
    "# Count subjects per subtype\n",
    "unique, counts = np.unique(ml_subtype, return_counts=True)\n",
    "print(\"\\nüë• Subjects per subtype:\")\n",
    "for subtype, count in zip(unique, counts):\n",
    "    print(f\"   ‚Ä¢ Subtype {subtype}: {count} subjects ({count/len(ml_subtype)*100:.1f}%)\")\n",
    "\n",
    "# Average stage per subtype\n",
    "print(\"\\nüìà Average disease stage per subtype:\")\n",
    "for subtype in unique:\n",
    "    mask = ml_subtype == subtype\n",
    "    avg_stage = ml_stage[mask].mean()\n",
    "    print(f\"   ‚Ä¢ Subtype {subtype}: Stage {avg_stage:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Save Results\n",
    "\n",
    "Export results for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'ml_subtype': ml_subtype,\n",
    "    'prob_ml_subtype': prob_ml_subtype,\n",
    "    'ml_stage': ml_stage,\n",
    "    'prob_ml_stage': prob_ml_stage\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "results_file = f\"{output_folder}/subject_assignments.csv\"\n",
    "results_df.to_csv(results_file, index=False)\n",
    "\n",
    "print(f\"‚úÖ Results saved to: {results_file}\")\n",
    "print(f\"\\nüìä First few rows:\")\n",
    "print(results_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Analysis Complete!\n",
    "\n",
    "### What you achieved:\n",
    "- ‚úÖ Ran OrdinalSustain with **2-4x speedup**\n",
    "- ‚úÖ Used **process-based parallel MCMC** (escapes Python's GIL)\n",
    "- ‚úÖ Reduced **30-day run** to **~8-12 days**\n",
    "- ‚úÖ Saved **~18-22 days** of computation time!\n",
    "\n",
    "### Next steps:\n",
    "1. Review the MCMC diagnostic plots\n",
    "2. Determine optimal number of subtypes\n",
    "3. Interpret subtype progressions\n",
    "4. Run cross-validation if needed\n",
    "\n",
    "### Files generated:\n",
    "- `{output_folder}/pickle_files/` - SuStaIn model outputs\n",
    "- `{output_folder}/subject_assignments.csv` - Subject-level results\n",
    "- `{output_folder}/mcmc_diagnostics.png` - Diagnostic plots\n",
    "\n",
    "---\n",
    "\n",
    "**Questions?** Check the repository documentation or create an issue.\n",
    "\n",
    "**Repository:** https://github.com/Amelia3141/mphil\n",
    "\n",
    "**Branch:** `claude/optimize-sustain-speed-011CV4Lk8FuUjS6hZNj13WE3`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
